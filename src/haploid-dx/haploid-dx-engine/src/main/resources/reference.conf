# haploid-dx-engine reference configuration file

haploid.dx.engine {

	default-timeout = 90 s
	
	domain {

		#
		# Allow an 'Info' request to last as long as this timeout
		#
		info-timeout = 90 s 
		
		#
		# if entries get streamed too fast during creation of new actors wait for this duration before reapplying the current entry
		# these values should be equal or similar to those in section journal
		#
		pause-during-create = 15 ms
		
		#
		# if entries get streamed too fast during creation of new actors retry reapplying the current entry up to this number of times
		# this value should be higher than the retries-during-redo because system load could be much higher
		#
		retries-during-create = 4000
		
		#
		# delete the job specific directory created in operating.root-directory and all its contents if the job is succussfully completed
		#
		remove-job-directory-on-success = on
		
		#
		# delete the job specific directory created in operating.root-directory and all its contents if the job is succussfully completed
		#
		remove-job-directory-on-failure = off
		
		#
		# JAXB marshalling from and to XML and JSON
		#		
		marshalling {
		
			#
			# pretty-print marshaled output
			#	
			formatted-output-xml = on
		
			formatted-output-json = off
		
			#
			# Users of this library must add their classes with JAXB bindings here
			#
			custom-classes-with-jaxb-bindings = [] 
		
			#
			# Maintain a list of all classes that provide JAXB bindings
			#
			classes-with-jaxb-bindings = [ 
				com.ibm.haploid.dx.engine.event.Execute
				com.ibm.haploid.dx.engine.event.ReceiverEvent
				com.ibm.haploid.dx.engine.event.JobCreate
				com.ibm.haploid.dx.engine.event.TaskCreate
				com.ibm.haploid.dx.engine.event.TaskOperationResult
				com.ibm.haploid.dx.engine.event.DefaultExecutionResult
				com.ibm.haploid.dx.engine.event.OperationCreate
				com.ibm.haploid.dx.engine.event.OperationResult
				com.ibm.haploid.dx.engine.event.Reset
				com.ibm.haploid.dx.engine.event.ResetAck
				com.ibm.haploid.dx.engine.event.TaskOperationResult
				com.ibm.haploid.dx.engine.event.TaskResult
				com.ibm.haploid.dx.engine.event.JobResult
				com.ibm.haploid.dx.engine.event.ExecutionStartEvent
				com.ibm.haploid.dx.engine.domain.DomainObject
				com.ibm.haploid.dx.engine.domain.Engine
				com.ibm.haploid.dx.engine.domain.Monitor
				com.ibm.haploid.dx.engine.domain.JobMonitor
				com.ibm.haploid.dx.engine.domain.TaskMonitor
				com.ibm.haploid.dx.engine.domain.Job
				com.ibm.haploid.dx.engine.domain.JobResultDetail
				com.ibm.haploid.dx.engine.domain.JobSequenceTaskDetail
				com.ibm.haploid.dx.engine.domain.Task
				com.ibm.haploid.dx.engine.domain.TaskResultDetail
				com.ibm.haploid.dx.engine.domain.binding.StringListWrapper
				com.ibm.haploid.dx.engine.domain.binding.PropertiesWrapper
				com.ibm.haploid.dx.engine.domain.binding.Property
				com.ibm.haploid.dx.engine.domain.binding.ResultXML
				com.ibm.haploid.dx.engine.domain.binding.TestXML
				com.ibm.haploid.dx.engine.domain.binding.ThrowableXML
				com.ibm.haploid.dx.engine.domain.binding.SerializedXML
				com.ibm.haploid.dx.engine.domain.flow.DefaultExecutionResultDetail
				com.ibm.haploid.dx.engine.domain.operating.OperationMonitor
				com.ibm.haploid.dx.engine.domain.operating.OperatorMonitor
				com.ibm.haploid.dx.engine.domain.operating.Operation
				com.ibm.haploid.dx.engine.domain.operating.Operator
				com.ibm.haploid.dx.engine.domain.operating.OperationDetail
				com.ibm.haploid.dx.engine.domain.operating.OperationResultDetail
				com.ibm.haploid.dx.engine.domain.operating.ExternalOperationDetail
				com.ibm.haploid.dx.engine.domain.operating.util.MoveOperationDetail
				com.ibm.haploid.dx.engine.domain.operating.util.ScriptOperationDetail
				com.ibm.haploid.dx.engine.domain.test.JobNoDetail
				com.ibm.haploid.dx.engine.domain.test.TaskNoDetail
				com.ibm.haploid.dx.engine.domain.test.OperationNoDetail
			] 
			
		}
	
		operating {
		
			#
			# Root of the directory structure for all operators.
			#
			# Under this directory should be a tree like 
			# ./engine/jobs/<job-id>/tasks/<task-id>/operations/<operation-id>/input
			# ./engine/jobs/<job-id>/tasks/<task-id>/operations/<operation-id>/working
			# ./engine/jobs/<job-id>/tasks/<task-id>/operations/<operation-id>/output
			#
			root-directory = ${haploid.core.application-directory}
		
			#
			# Users of this library must add their own OperatorBase classes here with the same syntax as for 'operator-classes'
			#
			custom-operator-classes = [] 
		
			#
			# Maintain a list of all classes that provide OperatorBase functionality and the number of instances, 
			# the routing mechanism is a SmallestMailboxRouter(number-of-instances)
			#
			operator-classes = [		
				{ operator-class = com.ibm.haploid.dx.engine.domain.operating.util.MoveOperator, name = move, number-of-instances = 5, timeout = 10 s } 
				{ operator-class = com.ibm.haploid.dx.engine.domain.operating.util.ScriptOperator, name = script, number-of-instances = 10, timeout = 90 s } 	
			]
			
			#
			# Maximum size in bytes for the log messages of one(!) operation
			#
			max-logging-per-operation-size = 512 k
			
			#
			# Encode long outputs (stack traces, console, logging) with BASE46
			#
			encode-output-with-base64 = on
			
			#
			# this is OS and machine language settings depending
			# a list of possible values can be found here (column 'Java name') for example
			#   http://www.devsphere.com/mapping/docs/guide/encodings.html
			#
			console-charset-windows = Cp1250
			
			#
			# this is OS and machine depending, on Windows use \\ for path separators on Unix use /
			#
			script-base-command-line-windows = "c:\\windows\\system32\\cmd.exe /c"
			
			#
			# default extension for script files, OS depending
			#
			script-extension-windows = ".cmd"

			#
			# same for unix
			#
			console-charset-unix = ISO8859_1
			
			script-base-command-line-unix = ""
			
			script-extension-unix = ".ksh"
		
		}
		
		binding {
			allow-serialized-xml = on
			warn-on-serialized-xml = off
		}
		
	}
	
	event {
	
		use-compression-during-serialization = on
		
	}
	
	journal {
	
		#
		# the default system-wide journal
		#
	    journal-class = com.ibm.haploid.dx.engine.journal.JournalIOJournal
	
	  	fileprefix = dx-journal-

  		filesuffix = .data 
	
		#
		# the files in this directory are the "persistent store", they must be handled with care
		#
		data-directory = ${haploid.core.application-directory}/journal
		
		#
		# this directory contains an xml-file for each event journaled, they can be used for desaster recovery
		#
		xml-backup-directory = ${haploid.dx.engine.journal.data-directory}/xml-backup
		
		#
		# this should be on a different machine, they are used during 'archiving and compacting'
		#
		archive-directory = ${haploid.dx.engine.journal.data-directory}/archive
		
		#
		# this should be on a different machine, the files in this directory are a 'almost live backup'
		#
		replicate-directory = ${haploid.dx.engine.journal.data-directory}/replicate
		
		#
		# start a new journal file if the former arrives at this size
		#
		max-filesize = 2000 M
		
		#
		# buffer in memory before flushing to disk
		#
		max-batchsize = 2 M
		
		#
		# if the journal is idle files will be flushed and closed after this interval, they will be re-opened when needed
		#
		dispose-interval = 5 s
		
		#
		# if entries get streamed too fast during redo wait for this duration before reapplying the current entry
		#
		pause-during-redo = 15 ms
		
		#
		# if entries get streamed too fast during redo retry reapplying the current entry up to this number of times
		#
		retries-during-redo = 1000
		
		#
		# if redo fails for an entry after the above number of retries then delete this entry from the journal
		#
		remove-entry-if-retries-fail = on 
		
		#
		# log a debug message every n entries
		#
		log-modulo = 100
						
	}
	
}
